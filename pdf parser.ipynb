{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:51:20.622174Z",
     "start_time": "2024-12-12T20:51:20.607469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Set the working directory to your project's root directory on D drive\n",
    "os.chdir(\"D:\\Pycharm_projects\")\n",
    "\n"
   ],
   "id": "29ceaa89a4178d52",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-12T20:52:33.863058Z",
     "start_time": "2024-12-12T20:52:33.856064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from IPython.display import display as Markdown\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:52:34.939092Z",
     "start_time": "2024-12-12T20:52:34.617215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "# Verify Python environment and paths\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = \"D:\\\\Pycharm_projects\\\\mono.pdf\"  # Ensure the file path is correct\n",
    "\n",
    "# Initialize data list to store text from each page\n",
    "data = []\n",
    "\n",
    "with open(pdf_path, \"rb\") as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    text = \"\"\n",
    "\n",
    "    # Iterate through all pages and extract text\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        page_text = page.extract_text()\n",
    "\n",
    "        # Store each page's number and extracted text in the data variable\n",
    "        data.append({\n",
    "            'page_number': page_num + 1,  # Page numbers are 1-indexed\n",
    "            'page_content': page_text\n",
    "        })\n",
    "\n",
    "    # Print the first 500 characters of the extracted text from the first page\n",
    "    print(\"Text extracted successfully from the first page:\", data[0]['page_content'][:500])\n",
    "\n",
    "# Optional: Print the data variable (can be large, depending on the PDF content)\n",
    "# print(data)\n"
   ],
   "id": "f628a9535793c20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: D:\\Pycharm_projects\n",
      "Text extracted successfully from the first page: MONOPOLY \n",
      "Property Trading Game from Parker Brothers\" \n",
      "AGES 8+ \n",
      "2 to 8 Players \n",
      "Contents: Gameboard, 3 dice, tokens, 32 houses, I2 hotels, Chance \n",
      "and Community Chest cards, Title Deed cards, play money and a Banker's tray. \n",
      "Now there's a faster way to play MONOPOLY. Choose to play by \n",
      "the classic rules for buying, renting and selling properties or use the \n",
      "Speed Die to get into the action faster. If you've never played the classic \n",
      "MONOPOLY game, refer to the Classic Rules beginning on the next\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:52:36.052360Z",
     "start_time": "2024-12-12T20:52:35.556898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ],
   "id": "e339f1437e97964f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:52:36.413352Z",
     "start_time": "2024-12-12T20:52:36.387587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Assuming 'data' has already been populated with 'page_content' as explained before\n",
    "\n",
    "# Convert the data list of dictionaries into a list of Document objects\n",
    "documents = [Document(page_content=page['page_content']) for page in data]\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\n",
    "\n",
    "# Split the documents into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Print the first chunk as an example\n",
    "print(\"First chunk:\", chunks[0])\n"
   ],
   "id": "7baa4ea643eef1b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First chunk: page_content='MONOPOLY \n",
      "Property Trading Game from Parker Brothers\" \n",
      "AGES 8+ \n",
      "2 to 8 Players \n",
      "Contents: Gameboard, 3 dice, tokens, 32 houses, I2 hotels, Chance \n",
      "and Community Chest cards, Title Deed cards, play money and a Banker's tray. \n",
      "Now there's a faster way to play MONOPOLY. Choose to play by \n",
      "the classic rules for buying, renting and selling properties or use the \n",
      "Speed Die to get into the action faster. If you've never played the classic \n",
      "MONOPOLY game, refer to the Classic Rules beginning on the next page. \n",
      "If you already know how to play and want to use the Speed Die, just \n",
      "read the section below for the additional Speed Die rules. \n",
      "SPEED DIE RULES \n",
      "Learnins how to Play with the S~eed Die IS as \n",
      "/ \n",
      "fast as playing with i't. \n",
      "1. When starting the game, hand out an extra $1,000 to each player \n",
      "(two $5005 should work). The game moves fast and you'll need \n",
      "the extra cash to buy and build. \n",
      "2. Do not use the Speed Die until you've landed on or passed over \n",
      "GO for the first time. Once you collect that first $200 salary, you'll \n",
      "use the Speed Die for the rest of the game. This means that some \n",
      "players will start using the die before others. \n",
      "3. Once you start using the Speed Die, roll it along with the two \n",
      "white dice on your turn. Then do the following depending on \n",
      "what you rolled. \n",
      "1, 2, or 3: Add this number to the roll of the two white \n",
      "dice. You'll zoom around the board.'\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:52:39.081787Z",
     "start_time": "2024-12-12T20:52:37.701362Z"
    }
   },
   "cell_type": "code",
   "source": "!ollama list",
   "id": "763c577d32c84410",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED     \n",
      "mistral:latest             f974a74358d6    4.1 GB    23 hours ago    \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    25 hours ago    \n",
      "llama3.2:latest            a80c4f17acd5    2.0 GB    26 hours ago    \n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:52:57.704429Z",
     "start_time": "2024-12-12T20:52:39.089539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Ensure 'chunks' is correctly populated with Documents (not strings or dicts)\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,  # Ensure 'chunks' is valid\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),  # Check model validity\n",
    "    collection_name=\"local-rag\"\n",
    ")\n"
   ],
   "id": "9ab50cc4f3480e79",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T17:55:35.361780Z",
     "start_time": "2024-12-12T17:55:34.982247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ],
   "id": "fad2a4594f1047d9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T17:56:02.785352Z",
     "start_time": "2024-12-12T17:56:02.376799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "local_model = \"mistral:latest\"\n",
    "llm = ChatOllama(model=local_model)"
   ],
   "id": "9fd9608da359a2fe",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T17:56:04.119179Z",
     "start_time": "2024-12-12T17:56:04.114327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ],
   "id": "d8c4ab5723734a94",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T17:56:05.145272Z",
     "start_time": "2024-12-12T17:56:05.135875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(),\n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ],
   "id": "c30e80301156a8b7",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T17:56:06.205260Z",
     "start_time": "2024-12-12T17:56:06.200117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "id": "f54ab3395e01efa7",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T17:56:08.695430Z",
     "start_time": "2024-12-12T17:56:07.679139Z"
    }
   },
   "cell_type": "code",
   "source": "chain.invoke(\"what is banker\")",
   "id": "a47ff29c276ac437",
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "llama runner process has terminated: error loading model: unable to allocate backend buffer",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResponseError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mchain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwhat is banker\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3022\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   3020\u001B[0m context\u001B[38;5;241m.\u001B[39mrun(_set_config_context, config)\n\u001B[0;32m   3021\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 3022\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3023\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3024\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, \u001B[38;5;28minput\u001B[39m, config)\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3727\u001B[0m, in \u001B[0;36mRunnableParallel.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   3722\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_executor_for_config(config) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[0;32m   3723\u001B[0m         futures \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m   3724\u001B[0m             executor\u001B[38;5;241m.\u001B[39msubmit(_invoke_step, step, \u001B[38;5;28minput\u001B[39m, config, key)\n\u001B[0;32m   3725\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key, step \u001B[38;5;129;01min\u001B[39;00m steps\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m   3726\u001B[0m         ]\n\u001B[1;32m-> 3727\u001B[0m         output \u001B[38;5;241m=\u001B[39m \u001B[43m{\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfuture\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfutures\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\n\u001B[0;32m   3728\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[0;32m   3729\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3727\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   3722\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_executor_for_config(config) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[0;32m   3723\u001B[0m         futures \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m   3724\u001B[0m             executor\u001B[38;5;241m.\u001B[39msubmit(_invoke_step, step, \u001B[38;5;28minput\u001B[39m, config, key)\n\u001B[0;32m   3725\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key, step \u001B[38;5;129;01min\u001B[39;00m steps\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m   3726\u001B[0m         ]\n\u001B[1;32m-> 3727\u001B[0m         output \u001B[38;5;241m=\u001B[39m {key: \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m key, future \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(steps, futures)}\n\u001B[0;32m   3728\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[0;32m   3729\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mC:\\Python311\\Lib\\concurrent\\futures\\_base.py:456\u001B[0m, in \u001B[0;36mFuture.result\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    454\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[1;32m--> 456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    458\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "File \u001B[1;32mC:\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[0;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[0;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[0;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Python311\\Lib\\concurrent\\futures\\thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3711\u001B[0m, in \u001B[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001B[1;34m(step, input, config, key)\u001B[0m\n\u001B[0;32m   3709\u001B[0m context \u001B[38;5;241m=\u001B[39m copy_context()\n\u001B[0;32m   3710\u001B[0m context\u001B[38;5;241m.\u001B[39mrun(_set_config_context, child_config)\n\u001B[1;32m-> 3711\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3712\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3713\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3714\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchild_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3715\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain_core\\retrievers.py:266\u001B[0m, in \u001B[0;36mBaseRetriever.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    265\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_retriever_error(e)\n\u001B[1;32m--> 266\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    267\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    268\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_retriever_end(\n\u001B[0;32m    269\u001B[0m         result,\n\u001B[0;32m    270\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain_core\\retrievers.py:259\u001B[0m, in \u001B[0;36mBaseRetriever.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    257\u001B[0m _kwargs \u001B[38;5;241m=\u001B[39m kwargs \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expects_other_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_arg_supported:\n\u001B[1;32m--> 259\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_relevant_documents\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    260\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_kwargs\u001B[49m\n\u001B[0;32m    261\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    263\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_relevant_documents(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_kwargs)\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain\\retrievers\\multi_query.py:168\u001B[0m, in \u001B[0;36mMultiQueryRetriever._get_relevant_documents\u001B[1;34m(self, query, run_manager)\u001B[0m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_relevant_documents\u001B[39m(\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    156\u001B[0m     query: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m    158\u001B[0m     run_manager: CallbackManagerForRetrieverRun,\n\u001B[0;32m    159\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Document]:\n\u001B[0;32m    160\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get relevant documents given a user query.\u001B[39;00m\n\u001B[0;32m    161\u001B[0m \n\u001B[0;32m    162\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    166\u001B[0m \u001B[38;5;124;03m        Unique union of relevant documents from all generated queries\u001B[39;00m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 168\u001B[0m     queries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_queries\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minclude_original:\n\u001B[0;32m    170\u001B[0m         queries\u001B[38;5;241m.\u001B[39mappend(query)\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain\\retrievers\\multi_query.py:185\u001B[0m, in \u001B[0;36mMultiQueryRetriever.generate_queries\u001B[1;34m(self, question, run_manager)\u001B[0m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_queries\u001B[39m(\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28mself\u001B[39m, question: \u001B[38;5;28mstr\u001B[39m, run_manager: CallbackManagerForRetrieverRun\n\u001B[0;32m    176\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m    177\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Generate queries based upon user input.\u001B[39;00m\n\u001B[0;32m    178\u001B[0m \n\u001B[0;32m    179\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;124;03m        List of LLM generated queries that are similar to the user input\u001B[39;00m\n\u001B[0;32m    184\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 185\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    186\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquestion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcallbacks\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\n\u001B[0;32m    187\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    188\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_chain, LLMChain):\n\u001B[0;32m    189\u001B[0m         lines \u001B[38;5;241m=\u001B[39m response[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3024\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   3022\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, \u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   3023\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3024\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3025\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[0;32m   3026\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[1;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    277\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    281\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    282\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[0;32m    283\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[0;32m    284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[0;32m    285\u001B[0m         ChatGeneration,\n\u001B[1;32m--> 286\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    287\u001B[0m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    288\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcallbacks\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtags\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_name\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    296\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    778\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    779\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    780\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    784\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    785\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 786\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    641\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[0;32m    642\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[1;32m--> 643\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    644\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    645\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[0;32m    646\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[0;32m    647\u001B[0m ]\n\u001B[0;32m    648\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    632\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 633\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    635\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    636\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    637\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    638\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    639\u001B[0m         )\n\u001B[0;32m    640\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    641\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    849\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    850\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 851\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    852\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    853\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    854\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    855\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:690\u001B[0m, in \u001B[0;36mChatOllama._generate\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    683\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate\u001B[39m(\n\u001B[0;32m    684\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    685\u001B[0m     messages: List[BaseMessage],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    688\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    689\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatResult:\n\u001B[1;32m--> 690\u001B[0m     final_chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_chat_stream_with_aggregation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    691\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    692\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    693\u001B[0m     generation_info \u001B[38;5;241m=\u001B[39m final_chunk\u001B[38;5;241m.\u001B[39mgeneration_info\n\u001B[0;32m    694\u001B[0m     chat_generation \u001B[38;5;241m=\u001B[39m ChatGeneration(\n\u001B[0;32m    695\u001B[0m         message\u001B[38;5;241m=\u001B[39mAIMessage(\n\u001B[0;32m    696\u001B[0m             content\u001B[38;5;241m=\u001B[39mfinal_chunk\u001B[38;5;241m.\u001B[39mtext,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    700\u001B[0m         generation_info\u001B[38;5;241m=\u001B[39mgeneration_info,\n\u001B[0;32m    701\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:591\u001B[0m, in \u001B[0;36mChatOllama._chat_stream_with_aggregation\u001B[1;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001B[0m\n\u001B[0;32m    582\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_chat_stream_with_aggregation\u001B[39m(\n\u001B[0;32m    583\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    584\u001B[0m     messages: List[BaseMessage],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    588\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    589\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatGenerationChunk:\n\u001B[0;32m    590\u001B[0m     final_chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 591\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_chat_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    592\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    593\u001B[0m \u001B[43m            \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mChatGenerationChunk\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    594\u001B[0m \u001B[43m                \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mAIMessageChunk\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    595\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    608\u001B[0m \u001B[43m                \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    609\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:578\u001B[0m, in \u001B[0;36mChatOllama._create_chat_stream\u001B[1;34m(self, messages, stop, **kwargs)\u001B[0m\n\u001B[0;32m    575\u001B[0m chat_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_chat_params(messages, stop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chat_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m--> 578\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mchat(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mchat_params)\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    580\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mchat(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mchat_params)\n",
      "File \u001B[1;32mD:\\Pycharm_projects\\.venv\\Lib\\site-packages\\ollama\\_client.py:167\u001B[0m, in \u001B[0;36mClient._request.<locals>.inner\u001B[1;34m()\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mHTTPStatusError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    166\u001B[0m   e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m--> 167\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m ResponseError(e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mtext, e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m r\u001B[38;5;241m.\u001B[39miter_lines():\n\u001B[0;32m    170\u001B[0m   part \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(line)\n",
      "\u001B[1;31mResponseError\u001B[0m: llama runner process has terminated: error loading model: unable to allocate backend buffer"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vector_db.delete_collection()",
   "id": "3b39f66b9570f133"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:44:09.269232Z",
     "start_time": "2024-12-13T05:43:48.456320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.chdir(\"D:\\Pycharm_projects\")\n",
    "\n",
    "\n",
    "pdf_path = \"D:\\\\Pycharm_projects\\\\mono.pdf\"\n",
    "data = []\n",
    "\n",
    "with open(pdf_path, \"rb\") as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        page_text = page.extract_text()\n",
    "\n",
    "        data.append({\n",
    "            'page_number': page_num + 1,\n",
    "            'page_content': page_text\n",
    "        })\n",
    "\n",
    "\n",
    "documents = [Document(page_content=page['page_content']) for page in data]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = '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'\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"huggingface-groq-rag\"\n",
    ")\n",
    "\n",
    "\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"mixtral-8x7b-32768\",\n",
    "    groq_api_key=os.environ['GROQ_API_KEY']\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI assistant generating alternative query perspectives.\n",
    "    Generate 5 different versions of the given question to improve document retrieval:\n",
    "    Original question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(),\n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "result = chain.invoke(\"summarise it\")\n",
    "print(result)\n"
   ],
   "id": "4b5062f3c6acff2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The documents provide rules for the game Monopoly. Each player starts with $1,500 and the Bank holds all remaining money and properties. The Bank pays salaries, sells properties, and collects taxes, fines, and loans. Players move around the board based on the roll of the dice, and may buy properties, pay rent, or perform other actions based on the space they land on. Players can buy houses and hotels to increase rent collection. Properties can be sold to other players or mortgaged through the Bank. There are special spaces on the board such as \"Free Parking,\" \"Chance,\" and \"Community Chest\" that allow players to draw cards and perform additional actions. The objective of the game is to become the wealthiest player through buying, renting, and selling properties.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ae430160f57f6f0a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
